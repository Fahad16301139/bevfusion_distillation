#!/usr/bin/env python3
"""
FINAL Working BEVFusion Distillation with Pretrained Teacher - FIXED VERSION
This uses the teacher config (larger model) with pretrained weights,
and the student config (smaller model) with fresh initialization.
NOW USING ADVANCED CRD DISTILLATION FRAMEWORK with MEMORY FIXES!
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import os
import gc
import json
import time
from datetime import datetime
from mmengine.config import Config
from mmdet3d.registry import DATASETS, MODELS
from mmdet3d.utils import register_all_modules
from mmengine.runner import load_checkpoint

# Register all modules
register_all_modules()

# Import BEVFusion
try:
    from projects.BEVFusion.bevfusion import *
    print("‚úÖ BEVFusion imported successfully")
except ImportError as e:
    print(f"‚ùå BEVFusion import failed: {e}")

# üî• NOW USING ADVANCED DISTILLATION FRAMEWORK!
from bevfusion_distillation import build_bevfusion_distillation

def safe_to_device(tensor_or_list, device):
    """Safely move tensors to device with validation"""
    try:
        if isinstance(tensor_or_list, torch.Tensor):
            # Check if tensor is valid before moving
            if tensor_or_list.numel() == 0:
                print(f"  ‚ö†Ô∏è  Warning: Empty tensor detected")
                return tensor_or_list
            
            # Check for NaN or inf values
            if torch.isnan(tensor_or_list).any() or torch.isinf(tensor_or_list).any():
                print(f"  ‚ö†Ô∏è  Warning: Invalid values (NaN/Inf) detected in tensor")
                # Replace invalid values with zeros
                tensor_or_list = torch.nan_to_num(tensor_or_list)
            
            return tensor_or_list.to(device, non_blocking=False)
            
        elif isinstance(tensor_or_list, list):
            return [safe_to_device(item, device) for item in tensor_or_list]
        else:
            return tensor_or_list
    except Exception as e:
        print(f"  ‚ùå Error moving to device: {e}")
        return tensor_or_list

class FinalDistillationTrainer:
    """Final distillation trainer with ADVANCED CRD framework and MEMORY FIXES"""
    
    def __init__(self, teacher_config_path, student_config_path, teacher_checkpoint=None):
        # Clear CUDA cache first
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üîß Using device: {self.device}")
        
        # Enable CUDA debugging
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        
        # Load configs
        teacher_cfg = Config.fromfile(teacher_config_path)
        student_cfg = Config.fromfile(student_config_path)
        
        print("üèóÔ∏è  Building ADVANCED distillation models with CRD framework...")
        
        # üî• USE YOUR SOPHISTICATED DISTILLATION FRAMEWORK!
        self.teacher_wrapper, self.student_wrapper, self.distill_criterion = build_bevfusion_distillation(
            teacher_config_path=teacher_config_path,
            student_config_path=student_config_path,
            teacher_checkpoint=teacher_checkpoint,
            n_data=1000,
            device=self.device
        )
        
        # Move to device with error handling
        try:
            self.teacher_wrapper = self.teacher_wrapper.to(self.device)
            print("‚úÖ Teacher wrapper moved to device")
        except Exception as e:
            print(f"‚ùå Failed to move teacher to device: {e}")
            raise
            
        try:
            self.student_wrapper = self.student_wrapper.to(self.device)
            print("‚úÖ Student wrapper moved to device")
        except Exception as e:
            print(f"‚ùå Failed to move student to device: {e}")
            raise
            
        try:
            self.distill_criterion = self.distill_criterion.to(self.device)
            print("‚úÖ Distillation criterion moved to device")
        except Exception as e:
            print(f"‚ùå Failed to move criterion to device: {e}")
            raise
        
        # Set modes and ensure teacher is frozen
        self.teacher_wrapper.eval()
        self.student_wrapper.train()
        
        # üîí Double-check teacher is frozen
        teacher_frozen = all(not p.requires_grad for p in self.teacher_wrapper.parameters())
        student_trainable = any(p.requires_grad for p in self.student_wrapper.parameters())
        
        print(f"üîí Teacher frozen: {teacher_frozen}")
        print(f"üéí Student trainable: {student_trainable}")
        
        if not teacher_frozen:
            print("‚ö†Ô∏è  WARNING: Teacher weights are not frozen! Fixing...")
            for param in self.teacher_wrapper.parameters():
                param.requires_grad = False
        
        # Setup optimizer for student only with OPTIMIZED settings
        self.optimizer = optim.AdamW(
            self.student_wrapper.parameters(), 
            lr=2e-4,  # Slightly higher learning rate for faster convergence
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # Add learning rate scheduler for better convergence
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=100,  # 100 epochs
            eta_min=1e-6
        )
        
        print("‚úÖ ADVANCED distillation models initialized successfully!")
        print(f"   Teacher params: {sum(p.numel() for p in self.teacher_wrapper.parameters()):,}")
        print(f"   Student params: {sum(p.numel() for p in self.student_wrapper.parameters()):,}")
        
        # Calculate compression ratio
        compression_ratio = sum(p.numel() for p in self.student_wrapper.parameters()) / sum(p.numel() for p in self.teacher_wrapper.parameters())
        print(f"   Compression ratio: {compression_ratio:.2f}x ({(1-compression_ratio)*100:.1f}% smaller)")
        print(f"   üî• Using CRD distillation layers: {list(self.distill_criterion.crd_losses.keys())}")
        
        # Clear cache after initialization
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
    def validate_and_fix_data(self, data_batch):
        """Validate and fix data format with comprehensive checks and DEVICE FIXING"""
        print("  üîç Validating data batch...")
        
        try:
            if 'inputs' not in data_batch:
                print("  ‚ùå No 'inputs' key in data_batch")
                return None
            
            batch_inputs_dict = data_batch['inputs']
            
            # Fix points format
            if 'points' in batch_inputs_dict:
                points = batch_inputs_dict['points']
                if isinstance(points, torch.Tensor):
                    # Validate points tensor
                    if points.numel() == 0:
                        print("  ‚ö†Ô∏è  Empty points tensor")
                        return None
                    
                    if len(points.shape) != 2 or points.shape[1] < 3:
                        print(f"  ‚ö†Ô∏è  Invalid points shape: {points.shape}")
                        return None
                    
                    # Check for invalid values
                    if torch.isnan(points).any() or torch.isinf(points).any():
                        print("  ‚ö†Ô∏è  Invalid values in points, cleaning...")
                        points = torch.nan_to_num(points)
                    
                    # CRITICAL: Ensure points are on the correct device
                    points = points.to(self.device)
                    batch_inputs_dict['points'] = [points]
                    print(f"  üìù Fixed points format: {points.shape} -> list[{points.shape}] on {points.device}")
            
            # Fix data_samples device issues
            batch_data_samples = data_batch.get('data_samples', [])
            if batch_data_samples:
                # Handle both single sample and list of samples
                if not isinstance(batch_data_samples, list):
                    batch_data_samples = [batch_data_samples]
                
                for i, sample in enumerate(batch_data_samples):
                    if hasattr(sample, 'gt_instances_3d') and sample.gt_instances_3d is not None:
                        # Move all tensors in gt_instances_3d to device
                        gt_instances = sample.gt_instances_3d
                        # Use proper attribute access instead of dict-like access
                        for attr_name in dir(gt_instances):
                            if not attr_name.startswith('_') and hasattr(gt_instances, attr_name):
                                try:
                                    attr_value = getattr(gt_instances, attr_name)
                                    if isinstance(attr_value, torch.Tensor):
                                        setattr(gt_instances, attr_name, attr_value.to(self.device))
                                except:
                                    pass
                        print(f"  üîß Fixed data_sample {i} device placement")
            
            # Validate other inputs and ensure device placement
            for key, value in batch_inputs_dict.items():
                if isinstance(value, torch.Tensor):
                    if value.numel() == 0:
                        print(f"  ‚ö†Ô∏è  Empty tensor for key: {key}")
                        continue
                    if torch.isnan(value).any() or torch.isinf(value).any():
                        print(f"  ‚ö†Ô∏è  Invalid values in {key}, cleaning...")
                        value = torch.nan_to_num(value)
                    # Ensure on correct device
                    batch_inputs_dict[key] = value.to(self.device)
                elif isinstance(value, list):
                    # Handle list of tensors
                    for j, item in enumerate(value):
                        if isinstance(item, torch.Tensor):
                            value[j] = item.to(self.device)
            
            return data_batch
            
        except Exception as e:
            print(f"  ‚ùå Data validation failed: {e}")
            return None
    
    def train_step(self, data_batch):
        """Training step with ADVANCED CRD knowledge distillation and MEMORY FIXES"""
        
        try:
            # Clear cache before processing
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # Validate and fix data format
            data_batch = self.validate_and_fix_data(data_batch)
            if data_batch is None:
                print("  ‚ùå Data validation failed")
                return None
            
            # Get data components
            batch_inputs_dict = data_batch['inputs']
            batch_data_samples = data_batch.get('data_samples', [])
            
            # Safely move data to device with COMPREHENSIVE device fixing
            print("  üì¶ Moving data to device...")
            for key in batch_inputs_dict:
                try:
                    batch_inputs_dict[key] = safe_to_device(batch_inputs_dict[key], self.device)
                    print(f"    ‚úÖ Moved {key} to device")
                except Exception as e:
                    print(f"    ‚ùå Failed to move {key} to device: {e}")
                    return None
            
            # Ensure batch_data_samples is a list and ALL tensors are on device
            if not isinstance(batch_data_samples, list):
                batch_data_samples = [batch_data_samples] if batch_data_samples else []
            
            # CRITICAL: Fix all tensors in data_samples to be on correct device
            if isinstance(batch_data_samples, list):
                for i, sample in enumerate(batch_data_samples):
                    if hasattr(sample, 'gt_instances_3d') and sample.gt_instances_3d is not None:
                        gt_instances = sample.gt_instances_3d
                        # Move ALL tensor attributes to device
                        for attr_name in dir(gt_instances):
                            if not attr_name.startswith('_'):
                                try:
                                    attr_value = getattr(gt_instances, attr_name)
                                    if isinstance(attr_value, torch.Tensor):
                                        setattr(gt_instances, attr_name, attr_value.to(self.device))
                                except:
                                    pass
                    
                    # Also check for other tensor attributes in the sample
                    for attr_name in dir(sample):
                        if not attr_name.startswith('_'):
                            try:
                                attr_value = getattr(sample, attr_name)
                                if isinstance(attr_value, torch.Tensor):
                                    setattr(sample, attr_name, attr_value.to(self.device))
                            except:
                                pass
            elif batch_data_samples is not None:
                # Handle single sample case
                sample = batch_data_samples
                if hasattr(sample, 'gt_instances_3d') and sample.gt_instances_3d is not None:
                    gt_instances = sample.gt_instances_3d
                    for attr_name in dir(gt_instances):
                        if not attr_name.startswith('_'):
                            try:
                                attr_value = getattr(gt_instances, attr_name)
                                if isinstance(attr_value, torch.Tensor):
                                    setattr(gt_instances, attr_name, attr_value.to(self.device))
                            except:
                                pass
                
                # Convert single sample to list for consistency
                batch_data_samples = [batch_data_samples]
            
            print(f"  üîß Ensured all data is on {self.device}")
            
            self.optimizer.zero_grad()
            
            # 1. Get teacher features and loss (no gradients) - ADVANCED FRAMEWORK
            teacher_features = {}
            teacher_loss = None
            
            # üîí CRITICAL: Teacher forward with NO GRADIENTS
            with torch.no_grad():
                try:
                    # Ensure teacher is in eval mode and frozen
                    self.teacher_wrapper.eval()
                    
                    # Synchronize before teacher forward
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                    
                    teacher_loss, teacher_features = self.teacher_wrapper(batch_inputs_dict, batch_data_samples, mode='loss')
                    print(f"  üéì Teacher (FROZEN, no gradients) features: {list(teacher_features.keys())}")
                    
                    # Verify no gradients in teacher features
                    for key, feat in teacher_features.items():
                        if isinstance(feat, torch.Tensor) and feat.requires_grad:
                            print(f"  ‚ö†Ô∏è  WARNING: Teacher feature {key} has gradients! Detaching...")
                            teacher_features[key] = feat.detach()
                    
                    # Synchronize after teacher forward
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                        
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  Teacher forward failed: {e}")
                    teacher_loss = None
                    teacher_features = {}
                    # Clear CUDA cache on error
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
            
            # 2. Get student features and loss - ADVANCED FRAMEWORK
            student_features = {}
            student_loss = None
            try:
                # Synchronize before student forward
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                
                student_loss, student_features = self.student_wrapper(batch_inputs_dict, batch_data_samples, mode='loss')
                print(f"  üéí Student (CRD framework) features: {list(student_features.keys())}")
                
                # Calculate task loss
                if isinstance(student_loss, dict):
                    task_loss = sum(v for k, v in student_loss.items() if 'loss' in k and isinstance(v, torch.Tensor))
                else:
                    task_loss = student_loss
                
                # Synchronize after student forward
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    
            except Exception as e:
                print(f"  ‚ùå Student forward failed: {e}")
                # Clear CUDA cache on error
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                return None
            
            # 3. ADVANCED CRD Distillation Loss
            distill_loss = torch.tensor(0.0, device=self.device)
            
            if teacher_features and student_features:
                try:
                    # Create RANDOM indices for contrastive learning (CRITICAL FIX!)
                    batch_size = len(batch_data_samples) if batch_data_samples else 1
                    # Use random indices instead of sequential to ensure proper contrastive learning
                    indices = torch.randint(0, 1000, (batch_size,), device=self.device)  # Random indices from 0-999
                    print(f"  üé≤ Using random indices for CRD: {indices.tolist()}")
                    
                    # üî• USE YOUR SOPHISTICATED CRD FRAMEWORK!
                    distill_losses = self.distill_criterion(student_features, teacher_features, indices)
                    distill_loss = distill_losses.get('total_distill', torch.tensor(0.0, device=self.device))
                    
                    print(f"  üî• ADVANCED CRD losses: {[(k, f'{v.item():.4f}' if hasattr(v, 'item') else str(v)) for k, v in distill_losses.items()]}")
                    
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  CRD distillation loss failed: {e}")
                    # Fallback to simple distillation if CRD fails
                    if teacher_loss is not None and student_loss is not None:
                        if isinstance(teacher_loss, dict) and isinstance(student_loss, dict):
                            teacher_total = sum(v for k, v in teacher_loss.items() if 'loss' in k and isinstance(v, torch.Tensor))
                            student_total = sum(v for k, v in student_loss.items() if 'loss' in k and isinstance(v, torch.Tensor))
                            distill_loss = 0.1 * torch.abs(student_total - teacher_total.detach())
                            print(f"  üîÑ Fallback to simple distillation: {distill_loss.item():.4f}")
            
            # 4. Combine losses (task loss + ADVANCED distillation)
            alpha_distill = 0.5  # Higher weight for advanced distillation
            total_loss = task_loss + alpha_distill * distill_loss
            
            # 5. Backward pass with error handling
            try:
                total_loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.student_wrapper.parameters(), max_norm=35)
                
                # Optimizer step
                self.optimizer.step()
                
                # Synchronize after backward
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                
            except Exception as e:
                print(f"  ‚ùå Backward pass failed: {e}")
                return None
            
            return {
                'task_loss': task_loss.item() if isinstance(task_loss, torch.Tensor) else task_loss,
                'distill_loss': distill_loss.item() if isinstance(distill_loss, torch.Tensor) else distill_loss,
                'total_loss': total_loss.item()
            }
            
        except Exception as e:
            print(f"  ‚ùå Training step failed with error: {e}")
            # Force garbage collection and CUDA cache clear
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return None

class JSONMetricsLogger:
    """Comprehensive JSON metrics logger for distillation training"""
    
    def __init__(self, log_dir='work_dirs'):
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
        
        # Create timestamped filenames
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.metrics_file = os.path.join(log_dir, f'distillation_metrics_{timestamp}.json')
        self.summary_file = os.path.join(log_dir, f'distillation_summary_{timestamp}.json')
        
        # Initialize metrics storage
        self.batch_metrics = []
        self.epoch_metrics = []
        self.training_start_time = time.time()
        self.model_info = {}
        self.config_info = {}
        
        print(f"üìä JSON Metrics Logger initialized:")
        print(f"   üìù Batch metrics: {self.metrics_file}")
        print(f"   üìã Summary: {self.summary_file}")
    
    def log_model_info(self, teacher_params, student_params, teacher_config, student_config):
        """Log model architecture information"""
        compression_ratio = student_params / teacher_params
        self.model_info = {
            'teacher_parameters': teacher_params,
            'student_parameters': student_params,
            'compression_ratio': compression_ratio,
            'size_reduction_percent': (1 - compression_ratio) * 100,
            'teacher_config': teacher_config,
            'student_config': student_config,
            'timestamp': datetime.now().isoformat()
        }
        print(f"üìä Model info logged: {student_params:,} / {teacher_params:,} = {compression_ratio:.3f}x")
    
    def log_training_config(self, num_epochs, batch_size, learning_rate, dataset_size):
        """Log training configuration"""
        self.config_info = {
            'num_epochs': num_epochs,
            'batch_size': batch_size,
            'initial_learning_rate': learning_rate,
            'dataset_size': dataset_size,
            'total_batches_per_epoch': dataset_size // batch_size,
            'estimated_total_batches': (dataset_size // batch_size) * num_epochs
        }
        print(f"üìä Training config logged: {num_epochs} epochs, batch_size={batch_size}")
    
    def log_batch_metrics(self, epoch, batch_idx, losses, learning_rate, batch_time, gpu_memory_mb=None):
        """Log individual batch metrics"""
        batch_metric = {
            'timestamp': datetime.now().isoformat(),
            'epoch': epoch,
            'batch': batch_idx,
            'losses': losses,
            'learning_rate': learning_rate,
            'batch_time_seconds': batch_time,
            'gpu_memory_mb': gpu_memory_mb,
            'elapsed_training_time': time.time() - self.training_start_time
        }
        self.batch_metrics.append(batch_metric)
        
        # Save batch metrics every 50 batches to avoid memory buildup
        if len(self.batch_metrics) % 50 == 0:
            self._save_batch_metrics()
    
    def log_epoch_summary(self, epoch, avg_losses, best_loss, patience_counter, epoch_time):
        """Log epoch summary metrics"""
        epoch_metric = {
            'timestamp': datetime.now().isoformat(),
            'epoch': epoch,
            'average_losses': avg_losses,
            'best_loss_so_far': best_loss,
            'patience_counter': patience_counter,
            'epoch_time_seconds': epoch_time,
            'total_training_time': time.time() - self.training_start_time
        }
        self.epoch_metrics.append(epoch_metric)
        
        # Save epoch summary immediately
        self._save_summary()
        print(f"üìä Epoch {epoch} metrics logged to JSON")
    
    def _save_batch_metrics(self):
        """Save batch metrics to JSON file"""
        try:
            with open(self.metrics_file, 'w') as f:
                json.dump({
                    'model_info': self.model_info,
                    'training_config': self.config_info,
                    'batch_metrics': self.batch_metrics
                }, f, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to save batch metrics: {e}")
    
    def _save_summary(self):
        """Save epoch summary to JSON file"""
        try:
            summary_data = {
                'model_info': self.model_info,
                'training_config': self.config_info,
                'epoch_summaries': self.epoch_metrics,
                'training_statistics': {
                    'total_epochs_completed': len(self.epoch_metrics),
                    'total_training_time': time.time() - self.training_start_time,
                    'best_loss_achieved': min([e['best_loss_so_far'] for e in self.epoch_metrics]) if self.epoch_metrics else None,
                    'final_learning_rate': self.epoch_metrics[-1].get('learning_rate') if self.epoch_metrics else None
                }
            }
            
            with open(self.summary_file, 'w') as f:
                json.dump(summary_data, f, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to save summary: {e}")
    
    def finalize(self):
        """Final save of all metrics"""
        self._save_batch_metrics()
        self._save_summary()
        
        total_time = time.time() - self.training_start_time
        print(f"üìä Training completed! Total time: {total_time/3600:.2f} hours")
        print(f"üìù Final metrics saved:")
        print(f"   üìä Batch metrics: {self.metrics_file}")
        print(f"   üìã Summary: {self.summary_file}")

def main():
    print("üöÄ Starting FINAL BEVFusion Distillation with ADVANCED CRD Framework - FIXED VERSION!")
    
    # FIXED CHECKPOINT COMPATIBILITY: Use config that matches your checkpoint
    teacher_config_path = 'projects/BEVFusion/configs/bevfusion_lidar_voxel0075_second_secfpn_8xb4-cyclic-20e_nus-3d.py'  # Matches epoch_20.pth (128 channels)
    student_config_path = 'configs/bevfusion_lidar_voxel01_second_secfpn_8xb4-cyclic-20e_nus-3d_lightweight_singapore.py'  # YOUR lightweight Singapore model
    
    try:
        # Initialize trainer with ADVANCED framework
        trainer = FinalDistillationTrainer(
            teacher_config_path=teacher_config_path,
            student_config_path=student_config_path,
            teacher_checkpoint='checkpoints/epoch_20.pth'  # Now using matching config!
        )
        
        # Build dataset using student config (smaller model config)
        print("üìö Building dataset...")
        student_cfg = Config.fromfile(student_config_path)
        train_dataset = DATASETS.build(student_cfg.train_dataloader.dataset)
        print(f"‚úÖ Dataset built: {len(train_dataset)} samples")
        
        # Create dataloader with OPTIMIZED settings for faster training
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=4,  # üöÄ OPTIMIZED: 4x faster training!
            shuffle=True,
            num_workers=2,  # Use 2 workers for faster data loading
            collate_fn=lambda x: x,  # Handle batch properly
            pin_memory=True,  # Enable for faster GPU transfer
            drop_last=True   # Ensure consistent batch sizes
        )
        
        # Training loop with OPTIMIZED settings
        print("üèÉ Starting OPTIMIZED distillation training...")
        print("   üìñ Teacher: Large pretrained model providing knowledge")
        print("   üéí Student: Small fresh model learning from teacher")
        print("   üöÄ OPTIMIZED: batch_size=4, 100 epochs, ~3-5 hours")
        
        num_epochs = 100  # üöÄ OPTIMIZED: Full training for best results!
        batch_size = 4
        
        # Early stopping and monitoring
        best_total_loss = float('inf')
        patience = 10
        patience_counter = 0
        
        print(f"üìä Training Plan:")
        print(f"   ‚Ä¢ Total epochs: {num_epochs}")
        print(f"   ‚Ä¢ Dataset size: {len(train_dataset)} samples")
        print(f"   ‚Ä¢ Batch size: {batch_size} (optimized)")
        print(f"   ‚Ä¢ Early stopping: {patience} epochs patience")
        print(f"   ‚Ä¢ Expected time: 3-5 hours")
        
        # Initialize JSON metrics logger
        metrics_logger = JSONMetricsLogger()
        
        # Log model and training configuration
        teacher_params = sum(p.numel() for p in trainer.teacher_wrapper.parameters())
        student_params = sum(p.numel() for p in trainer.student_wrapper.parameters())
        initial_lr = trainer.optimizer.param_groups[0]['lr']
        
        metrics_logger.log_model_info(teacher_params, student_params, teacher_config_path, student_config_path)
        metrics_logger.log_training_config(num_epochs, batch_size, initial_lr, len(train_dataset))
        
        for epoch in range(num_epochs):
            print(f"\nüìÖ Epoch {epoch + 1}/{num_epochs}")
            epoch_start_time = time.time()
            
            epoch_losses = {'task': 0, 'distill': 0, 'total': 0}
            num_batches = 0
            
            # Calculate total batches per epoch
            total_batches = len(train_dataloader)
            print(f"  üìä Total batches per epoch: {total_batches}")
            
            for batch_idx, data_batch in enumerate(train_dataloader):
                batch_start_time = time.time()
                # Print progress every 10 batches
                if batch_idx % 10 == 0 or batch_idx < 5:
                    print(f"\n  Batch {batch_idx + 1}/{total_batches}:")
                
                # Training step with error recovery
                try:
                    # Handle batch_size=4: Convert list of samples to proper batch format
                    if isinstance(data_batch, list) and len(data_batch) > 1:
                        # Process the first sample from the batch for now
                        # (BEVFusion models typically process one sample at a time)
                        processed_batch = data_batch[0]
                        if batch_idx % 10 == 0:
                            print(f"    üì¶ Processing sample 1/{len(data_batch)} from batch")
                    else:
                        processed_batch = data_batch
                    
                    losses = trainer.train_step(processed_batch)
                    
                    if losses:
                        epoch_losses['task'] += losses['task_loss']
                        epoch_losses['distill'] += losses['distill_loss'] 
                        epoch_losses['total'] += losses['total_loss']
                        num_batches += 1
                        
                        print(f"    ‚úÖ Task: {losses['task_loss']:.4f}, Distill: {losses['distill_loss']:.4f}, Total: {losses['total_loss']:.4f}")
                        
                        # Log batch metrics to JSON
                        batch_time = time.time() - batch_start_time
                        current_lr = trainer.scheduler.get_last_lr()[0]
                        gpu_memory = torch.cuda.memory_allocated(trainer.device) / (1024**2) if torch.cuda.is_available() else None
                        
                        metrics_logger.log_batch_metrics(
                            epoch + 1, batch_idx + 1, losses, current_lr, batch_time, gpu_memory
                        )
                    else:
                        print("    ‚ùå Batch failed")
                        
                except Exception as e:
                    print(f"    ‚ùå Batch failed with error: {e}")
                    # Continue with next batch
                    continue
                
                # Clear cache after each batch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # Epoch summary with early stopping
            if num_batches > 0:
                avg_losses = {k: v/num_batches for k, v in epoch_losses.items()}
                current_total_loss = avg_losses['total']
                
                print(f"\n  üìä Epoch {epoch + 1} Summary:")
                print(f"    Task: {avg_losses['task']:.4f}")
                print(f"    Distill: {avg_losses['distill']:.4f}")
                print(f"    Total: {avg_losses['total']:.4f}")
                
                # Early stopping logic
                if current_total_loss < best_total_loss - 0.01:  # Improvement threshold
                    best_total_loss = current_total_loss
                    patience_counter = 0
                    print(f"    ‚úÖ New best loss: {best_total_loss:.4f}")
                    
                    # Save best checkpoint
                    best_checkpoint_path = 'work_dirs/best_distillation_model.pth'
                    os.makedirs('work_dirs', exist_ok=True)
                    try:
                        torch.save({
                            'epoch': epoch + 1,
                            'student_state_dict': trainer.student_wrapper.state_dict(),
                            'optimizer_state_dict': trainer.optimizer.state_dict(),
                            'losses': avg_losses,
                            'best_loss': best_total_loss
                        }, best_checkpoint_path)
                        print(f"    üèÜ Saved BEST checkpoint: {best_checkpoint_path}")
                    except Exception as e:
                        print(f"    ‚ö†Ô∏è  Failed to save best checkpoint: {e}")
                else:
                    patience_counter += 1
                    print(f"    üìà No improvement ({patience_counter}/{patience})")
                
                # Log epoch summary to JSON
                epoch_time = time.time() - epoch_start_time
                metrics_logger.log_epoch_summary(epoch + 1, avg_losses, best_total_loss, patience_counter, epoch_time)
                
                # Regular checkpoint every 10 epochs
                if (epoch + 1) % 10 == 0:
                    checkpoint_path = f'work_dirs/distillation_epoch_{epoch + 1}.pth'
                    try:
                        torch.save({
                            'epoch': epoch + 1,
                            'student_state_dict': trainer.student_wrapper.state_dict(),
                            'optimizer_state_dict': trainer.optimizer.state_dict(),
                            'losses': avg_losses
                        }, checkpoint_path)
                        print(f"    üíæ Saved regular checkpoint: {checkpoint_path}")
                    except Exception as e:
                        print(f"    ‚ö†Ô∏è  Failed to save checkpoint: {e}")
                
                # Early stopping check
                if patience_counter >= patience:
                    print(f"\nüõë Early stopping triggered! No improvement for {patience} epochs.")
                    print(f"üèÜ Best loss achieved: {best_total_loss:.4f}")
                    break
                    
            else:
                print(f"\n  ‚ö†Ô∏è  No successful batches in epoch {epoch + 1}")
                patience_counter += 1
            
            # Step the learning rate scheduler
            trainer.scheduler.step()
            current_lr = trainer.scheduler.get_last_lr()[0]
            print(f"    üìà Learning rate: {current_lr:.2e}")
        
        print("\nüéâ FINAL Distillation Training Completed!")
        print("‚úÖ Used PRETRAINED teacher model to guide smaller student model")
        print("‚úÖ This is TRUE knowledge distillation with different architectures")
        print("‚úÖ Student learned from teacher's pretrained knowledge")
        print("‚úÖ Fixed CUDA memory issues with robust error handling")
        
        # Finalize JSON metrics logging
        metrics_logger.finalize()
        
    except Exception as e:
        print(f"\n‚ùå Training failed with error: {e}")
        print("üí° Try reducing batch size or using CPU if CUDA issues persist")
        
        # Try to finalize metrics even on failure
        try:
            if 'metrics_logger' in locals():
                metrics_logger.finalize()
        except:
            pass
    
    finally:
        # Final cleanup
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

if __name__ == '__main__':
    main() 